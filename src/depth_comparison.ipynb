{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "import glob\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories = glob.glob('.experiments/end-effector_tuning/*/*num_hiddens-2*/result*.pth')\n",
    "directories = glob.glob('.experiments/end-effector_tune_beta/*/*/result*.pth')\n",
    "#load each \n",
    "results = {}\n",
    "seed = 0\n",
    "is_one_model = False\n",
    "for pth in directories:\n",
    "    act = pth.split('/')[-2].split('-activation-')[-1].split('-')[0]\n",
    "    hid_lay = pth.split('/')[-2].split('-num_hiddens-')[-1].split('-')[0]\n",
    "\n",
    "    #algorithm = pth.split('/')[2] + '-' + dt\n",
    "    #algorithm = act + '-' + hid_lay\n",
    "    algorithm = \"beta:\" + pth.split('/')[-2].split('-beta-')[-1].split('-')[0]\n",
    "    result = torch.load(pth)\n",
    "    if algorithm in results and not is_one_model:\n",
    "        start = len(results[algorithm])\n",
    "        end = len(results[algorithm]) + len(result)\n",
    "        j = 0\n",
    "        for i in range(start, end):\n",
    "            results[algorithm][i] = result[j]\n",
    "            j += 1        \n",
    "    elif algorithm in results and is_one_model:\n",
    "        results[algorithm + \"-\" + str(seed)] = result\n",
    "        seed += 1\n",
    "    else:\n",
    "        results[algorithm] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results =OrderedDict(sorted(results.items(), key=lambda t: t[0]))\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Error Curves\n",
    "In this section we want to see if our Jacobian estimates with our neural network actually solve the undlerying task of interest (i.e. moving the end effector to some target position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_eps_errs(result, dim=-1):\n",
    "    err_over_time = []\n",
    "    for k, value in result.items():\n",
    "        episode_err = []\n",
    "        for v in value:\n",
    "            state = v[0]\n",
    "            if dim < 0:\n",
    "                psn = state[0:3]\n",
    "                targ = state[-3:]\n",
    "                mse = np.linalg.norm(targ - psn, 2)\n",
    "                rmse = np.sqrt(mse)\n",
    "                \n",
    "                #mse = rmse\n",
    "            else:\n",
    "                psn = state[dim]\n",
    "                targ = state[-3 + dim]\n",
    "                mse = np.sqrt((targ - psn) ** 2)\n",
    "            episode_err.append(mse)\n",
    "        err_over_time.append(episode_err)\n",
    "    return np.array(err_over_time).T\n",
    "\n",
    "def plot_mu_sig(data, label=None, axis=1, ax=None):\n",
    "    mean = np.array(data).mean(axis=axis)\n",
    "    std = np.array(data).std(axis=axis)\n",
    "    ste = std / np.sqrt(data.shape[1])\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.plot(mean, label=label)\n",
    "        plt.fill_between(list(range(mean.shape[0])), mean + std, mean - std, alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(mean, label=label)\n",
    "        ax.fill_between(list(range(mean.shape[0])), mean + ste, mean - ste, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_curr_res = {}\n",
    "for algorithm, result in results.items():\n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    err_curr_res[algorithm] = err_over_time\n",
    "    #plot_mu_sig(err_over_time, label=algorithm, ax=ax)\n",
    "    \n",
    "bounds = [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "fig, axs = plt.subplots(len(bounds) - 1, 1, figsize=(15, 30))\n",
    "i = 0\n",
    "for start, end in zip(bounds[:-1], bounds[1:]):\n",
    "    ax = axs[i]\n",
    "    print(ax)\n",
    "    for algorithm in results.keys():\n",
    "        err_over_time = err_curr_res[algorithm]\n",
    "        to_plot = np.logical_and(err_over_time[0, :] >= start, err_over_time[0, :] < end)\n",
    "        plot_mu_sig(err_over_time[:, to_plot], label=algorithm +\": {}\".format(to_plot.sum()), ax=ax)\n",
    "    \n",
    "    \n",
    "    ax.get_yaxis().set_ticks(np.linspace(0.0, end, 5))\n",
    "    ax.legend()\n",
    "    ax.set_title(\"start distance: {} - {}\".format(start, end))\n",
    "    if i == len(results):\n",
    "        ax.set_xlabel(\"start mean squared error\")\n",
    "    else:\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    \n",
    "    ax.set_ylabel(\"Error Decrease\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [str(i) for i in range(3)]\n",
    "fig, axs = plt.subplots(len(titles), 1, figsize=(15, 25))\n",
    "for i, ax in enumerate(axs):\n",
    "    for algorithm, result in results.items():\n",
    "        err_over_time = collect_eps_errs(result, dim=i)\n",
    "        plot_mu_sig(err_over_time, label=algorithm, ax=ax)\n",
    "        ax.set_title(titles[i])\n",
    "        ax.legend()\n",
    "        if i == 2:\n",
    "            ax.set_xlabel(\"Time step\")\n",
    "        ax.set_ylabel(\"Error Decrease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising start and end points for Trajectories\n",
    "\n",
    "In this section we visualize the starting position against the end position. The intention of these plots is to understand how distance to the target might affect convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "\n",
    "for algorithm, result in results.items():\n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    \n",
    "    start = err_over_time[0,:]\n",
    "    end = err_over_time[-1,:]\n",
    "    ax.scatter(start, end, label=algorithm)\n",
    "    i += 1\n",
    "    #ax.set_title(algorithm)\n",
    "    ax.legend()\n",
    "    ax.get_yaxis().set_ticks([0.0, 1.0,  2.0])\n",
    "    ax.set_xlabel(\"start distance\")\n",
    "    ax.set_ylabel(\"end distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(len(results), 1, figsize=(15, 15))\n",
    "i = 0\n",
    "for algorithm, result in results.items():\n",
    "    ax = axs[i]\n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    \n",
    "    start = err_over_time[0,:]\n",
    "    end = err_over_time[-1,:]\n",
    "    ax.scatter(start, end, label=algorithm)\n",
    "    i += 1\n",
    "    #ax.set_title(algorithm)\n",
    "    ax.legend()\n",
    "    ax.get_yaxis().set_ticks([float(i)*0.1 for i in range(0, 20, 4)])\n",
    "    if i == len(results):\n",
    "        ax.set_xlabel(\"start mean squared error\")\n",
    "    else:\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "    ax.set_ylabel(\"end mean squared error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots of Area under the Curve\n",
    "\n",
    "In this section we summarise the above error plots by looking at the AUC for each trajectory. Smaller AUC score are indicative of faster convergence to the target. This also gives us way to compare different algorithms a bit more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 9))\n",
    "\n",
    "labels = []\n",
    "aucs = []\n",
    "for algorithm, result in results.items():\n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    #Here, we WANT to sum over each run\n",
    "    #above we do not because that is less meaninful\n",
    "    auc = err_over_time.sum(axis=0)   # for delta t between each timestep\n",
    "    print(auc.shape)\n",
    "    aucs.append(auc)\n",
    "    labels.append(algorithm)\n",
    "    \n",
    "ax.violinplot(aucs, showmeans=False, showmedians=True)\n",
    "ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_ylabel(\"AUC of Trajectories\")\n",
    "ax.set_title(\"AUC of Trajectories for Different UVS Algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(20, 20))\n",
    "\n",
    "bounds = [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "i = 0\n",
    "violin_results = {}\n",
    "for algorithm, result in results.items():\n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    violin_results[algorithm] = err_over_time\n",
    "\n",
    "for low, high in zip(bounds[:-1], bounds[1:]):\n",
    "    labels = []\n",
    "    aucs = []\n",
    "    count = []\n",
    "    for algorithm, result in violin_results.items():\n",
    "        err_over_time = result#collect_eps_errs(result, dim=-1)\n",
    "        #Here, we WANT to sum over each run\n",
    "        #above we do not because that is less meaninful\n",
    "        \n",
    "        to_plot = np.logical_and(err_over_time[0, :] >= low, err_over_time[0, :] < high)\n",
    "        count.append(to_plot.sum())\n",
    "        \n",
    "        to_plot = np.logical_and(to_plot, err_over_time[-1,:] <= .1)\n",
    "        auc = err_over_time.sum(axis=0)   # for delta t between each timestep\n",
    "        auc = auc[to_plot]\n",
    "        if len(auc) == 0:\n",
    "            auc = np.zeros(err_over_time.shape[-1])\n",
    "        #print(auc.shape, low, high, to_plot.sum())\n",
    "        aucs.append(auc)\n",
    "        if \"global-neuralnetwork\" in algorithm:\n",
    "            labels.append(algorithm.split(\"global-\")[-1])\n",
    "        else:\n",
    "            labels.append(algorithm)\n",
    "    ax = axs[i]\n",
    "    i+= 1\n",
    "    ax.violinplot(aucs, showmeans=False, showmedians=True)\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "    ax.set_xticklabels([l + \" {}/{}\".format(len(a),c) for l, a, c in zip(labels, aucs,count)])\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_ylabel(\"AUC of trajectory\")\n",
    "    ax.set_title(\"Initial distance: {} - {} meters\".format(low, high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 9))\n",
    "\n",
    "labels = []\n",
    "aucs = []\n",
    "for algorithm, result in results.items():\n",
    "    err_over_time = collect_eps_errs(result, dim=-1) \n",
    "    #want to sum over each trajectory\n",
    "    auc = err_over_time.sum(axis=0)  #for delta t between each time step\n",
    "    aucs.append(auc)\n",
    "    labels.append(algorithm)\n",
    "    \n",
    "ax.boxplot(aucs, labels=labels)\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_ylabel(\"AUC of Trajectories\")\n",
    "ax.set_title(\"AUC of Trajectories for Different UVS Algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Jacobians\n",
    "\n",
    "Here, we just calaculate the difference between Jacobians in trajectories to see how much they differ from the true underlying Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_jacobian_dist(result, norm=None):\n",
    "    err_over_time = []\n",
    "    for k, value in result.items():\n",
    "        episode_err = []\n",
    "        for v in value:\n",
    "            estimate_J = v[-2] \n",
    "\n",
    "            true_J = v[-1]\n",
    "            if norm is not \"signs\":\n",
    "                mse = np.linalg.norm(estimate_J - true_J, norm) #, 'fro')\n",
    "            else:\n",
    "                sign_mismatch = np.logical_not(np.logical_and(estimate_J, true_J))\n",
    "                mse = (sign_mismatch).sum() #techinically not mind you...\n",
    "            episode_err.append(mse)\n",
    "        err_over_time.append(episode_err)    \n",
    "    \n",
    "    return np.array(err_over_time).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing only on the frobenius norm\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "for algorithm, result in results.items():\n",
    "    jac_err = collect_jacobian_dist(result)\n",
    "\n",
    "    plot_mu_sig(jac_err, label=algorithm, axis=1, ax=ax)\n",
    "    \n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Jacobian distance')\n",
    "ax.set_title(\"Frobenius Norm Distance\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing only on the frobenius norm\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 20))\n",
    "\n",
    "activations = ['tanh', 'sigmoid', 'relu']\n",
    "\n",
    "plot_to_use = {k: v for k,v in zip(activations, ax)}\n",
    "\n",
    "for algorithm, result in results.items():\n",
    "    act = algorithm.split('-')[0]\n",
    "    jac_err = collect_jacobian_dist(result)\n",
    "    ax = plot_to_use[act]\n",
    "    plot_mu_sig(jac_err, label=algorithm, axis=1, ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('Jacobian distance')\n",
    "    ax.set_title(\"Frobenius Norm Distance\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing if differen't norms highlight any othernotable differences\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10, 20))\n",
    "norms = ['fro', 'nuc', np.inf, -np.inf]\n",
    "for i, ax in enumerate(axs):\n",
    "    n = norms[i]\n",
    "    for algorithm, result in results.items():\n",
    "        jac_err = collect_jacobian_dist(result, norm=n)\n",
    "        plot_mu_sig(jac_err, label=algorithm, axis=1, ax=ax)\n",
    "    if i == 3:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('Jacobian distance')\n",
    "    ax.set_title(n)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing only on the sign  norm\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "for algorithm, result in results.items():\n",
    "    jac_err = collect_jacobian_dist(result, norm='signs')\n",
    "    plot_mu_sig(jac_err, label=algorithm, axis=1, ax=ax)\n",
    "    \n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Jacobian distance')\n",
    "ax.set_title(\"Frobenius Norm result\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Criteria of Convergence\n",
    "The last thing I want to check for is for whether or not the critieria on J is met for the trajectories. I will need to review this to make sure I understand it, but if JJ^{+} has any non-positive eigen values (0 or less) then hypothetically, it should not converge.\n",
    "\n",
    "I think hypothetically JJ^{+} should be the identity matrix...and if it's not then we have problems otherwise, I'll need to read up on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pos_definite(A):\n",
    "    eigs = np.linalg.eigvals(A)\n",
    "            \n",
    "    return (eigs > 0).all()\n",
    "    \n",
    "def check_jacobian_positive_definite(result, invert_true_J=False):\n",
    "    err_over_time = []\n",
    "    for k, value in result.items():\n",
    "        episode_err = []\n",
    "        for v in value:\n",
    "            #Check against true jacobian\n",
    "            #If our approximation is good, presumably the matrix should still be positive definite\n",
    "            # i.e. the approximate J is transferable \n",
    "            J_hat = v[-2] \n",
    "            J = v[-1]\n",
    "            \n",
    "            if invert_true_J:\n",
    "                iJ_hat = np.linalg.pinv(J_hat)\n",
    "                JiJ = np.matmul(J, iJ_hat)\n",
    "            else:\n",
    "                iJ = np.linalg.pinv(J)\n",
    "                JiJ = np.matmul(J_hat, iJ)\n",
    "            is_pos_def = check_pos_definite(JiJ)\n",
    "            loss = int(is_pos_def)\n",
    "            episode_err.append(loss)\n",
    "        err_over_time.append(episode_err)    \n",
    "    \n",
    "    return np.array(err_over_time).T\n",
    "\n",
    "\n",
    "def check_jacobian_condition(result):\n",
    "    conditioning_over_time = []\n",
    "    true_solution_over_time = []\n",
    "    for k, value in result.items():\n",
    "        eps_conditioning = []\n",
    "        true_conditioning = []\n",
    "        for v in value:\n",
    "            #Check against true jacobian\n",
    "            #If our approximation is good, presumably the matrix should still be positive definite\n",
    "            # i.e. the approximate J is transferable \n",
    "            #this was backwards.....\n",
    "            J_hat = v[-1] \n",
    "            J = v[-2]\n",
    "            \n",
    "            #print(\"condition numbers:\")\n",
    "            #print(\"J hat\", np.linalg.cond(J_hat), \"J\", np.linalg.cond(J), \"JiJ\", np.linalg.cond(JiJ))\n",
    "            hat_cond = np.linalg.cond(J_hat)\n",
    "            true_cond = np.linalg.cond(J)\n",
    "            \n",
    "            #iJ_hat = np.linalg.pinv(J_hat)\n",
    "            #JiJ = np.matmul(J, iJ_hat)\n",
    "            \n",
    "            #JiJ_cond = np.linalg.cond(JiJ)\n",
    "            eps_conditioning.append(hat_cond)\n",
    "            true_conditioning.append(true_cond)\n",
    "            \n",
    "        conditioning_over_time.append(eps_conditioning)    \n",
    "        true_solution_over_time.append(true_conditioning)\n",
    "    return np.array(conditioning_over_time).T, np.array(true_solution_over_time).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking conditioning of Matrix over episodes & comparing to true jacobian at those points\n",
    "#Focusing only on the frobenius norm\n",
    "fig, axs = plt.subplots(len(results.keys()), 1, figsize=(15, 20))\n",
    "\n",
    "for i, k_v in enumerate(results.items()):\n",
    "    algorithm, result = k_v[0], k_v[1]\n",
    "    ax = axs[i]\n",
    "    cond_hat, cond_true = check_jacobian_condition(result)\n",
    "        \n",
    "    plot_mu_sig(cond_hat, label=\"approximation\", ax=ax)\n",
    "    #plot_mu_sig(cond_true, label=\"true values\", ax=ax)\n",
    "    if i >= len(axs) - 1:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('Condition Number')\n",
    "    ax.set_title(\"Checking Condition Number: {}\".format(algorithm))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Focusing only on the frobenius norm\n",
    "fig, axs = plt.subplots(len(results.keys()), 1, figsize=(15, 20))\n",
    "#if not isinstance(axs, list):\n",
    "#    axs = [axs]\n",
    "for i, k_v in enumerate(results.items()):\n",
    "    algorithm, result = k_v[0], k_v[1]\n",
    "    ax = axs[i]\n",
    "    jac_err = check_jacobian_positive_definite(result)\n",
    "    \n",
    "    #check if criteria is met for whole trajectory or not\n",
    "    always_pos_def = (jac_err.sum(axis=0) >=200)\n",
    "    not_always_pos_def = (jac_err.sum(axis=0) < 200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    traj_pos_def = err_over_time[:,always_pos_def]\n",
    "    traj_not_pos_def = err_over_time[:, not_always_pos_def]\n",
    "    \n",
    "    \n",
    "    total_traj = jac_err.shape[1]\n",
    "    total_pos_d = always_pos_def.sum()\n",
    "    total_not_pos_d = not_always_pos_def.sum()\n",
    "    \n",
    "    #plot_mu_sig(err_over_time, label=\"All {} Trajectories\".format(total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_pos_def, label=\"+ Definite {} / {}\".format(total_pos_d, total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_not_pos_def, label=\" Not + Definite {}/{}\".format(total_not_pos_d, total_traj), ax=ax)\n",
    "    if i >= len(axs) - 1:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('MSE over time')\n",
    "    ax.set_title(\"Positive Definite Criteria: {}\".format(algorithm))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing only on the frobenius norm\n",
    "fig, axs = plt.subplots(len(results.keys()), 1, figsize=(15, 20))\n",
    "#if not isinstance(axs, list):\n",
    "#    axs = [axs]\n",
    "for i, k_v in enumerate(results.items()):\n",
    "    algorithm, result = k_v[0], k_v[1]\n",
    "    ax = axs[i]\n",
    "    jac_err = check_jacobian_positive_definite(result, True)\n",
    "    \n",
    "    #check if criteria is met for whole trajectory or not\n",
    "    always_pos_def = (jac_err.sum(axis=0) >=100)\n",
    "    not_always_pos_def = (jac_err.sum(axis=0) < 100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    traj_pos_def = err_over_time[:,always_pos_def]\n",
    "    traj_not_pos_def = err_over_time[:, not_always_pos_def]\n",
    "    \n",
    "    \n",
    "    total_traj = jac_err.shape[1]\n",
    "    total_pos_d = always_pos_def.sum()\n",
    "    total_not_pos_d = not_always_pos_def.sum()\n",
    "    \n",
    "    #plot_mu_sig(err_over_time, label=\"All {} Trajectories\".format(total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_pos_def, label=\"+ Definite {} / {}\".format(total_pos_d, total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_not_pos_def, label=\" Not + Definite {}/{}\".format(total_not_pos_d, total_traj), ax=ax)\n",
    "    if i >= len(axs) - 1:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('MSE over time')\n",
    "    ax.set_title(\"Positive Definite Criteria: {}\".format(algorithm))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of positive definiteness \n",
    "here we isolate to specific depths to see how results compare. Just makes it easier to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing only on the frobenius norm\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 20))\n",
    "#if not isinstance(axs, list):\n",
    "#    axs = [axs]\n",
    "depth = str(8)\n",
    "plot_indx = 0\n",
    "for i, k_v in enumerate(results.items()):\n",
    "    algorithm, result = k_v[0], k_v[1]\n",
    "    if depth !=  algorithm.split('-')[1]:\n",
    "        continue\n",
    "    print(algorithm)\n",
    "    ax = axs[plot_indx]\n",
    "    plot_indx += 1\n",
    "    jac_err = check_jacobian_positive_definite(result)\n",
    "    \n",
    "    #check if criteria is met for whole trajectory or not\n",
    "    always_pos_def = (jac_err.sum(axis=0) >=100)\n",
    "    not_always_pos_def = (jac_err.sum(axis=0) < 100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    err_over_time = collect_eps_errs(result, dim=-1)\n",
    "    traj_pos_def = err_over_time[:,always_pos_def]\n",
    "    traj_not_pos_def = err_over_time[:, not_always_pos_def]\n",
    "    \n",
    "    \n",
    "    total_traj = jac_err.shape[1]\n",
    "    total_pos_d = always_pos_def.sum()\n",
    "    total_not_pos_d = not_always_pos_def.sum()\n",
    "    \n",
    "    #plot_mu_sig(err_over_time, label=\"All {} Trajectories\".format(total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_pos_def, label=\"+ Definite {} / {}\".format(total_pos_d, total_traj), ax=ax)\n",
    "    plot_mu_sig(traj_not_pos_def, label=\" Not + Definite {}/{}\".format(total_not_pos_d, total_traj), ax=ax)\n",
    "    if i >= len(axs) - 1:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('MSE over time')\n",
    "    ax.set_title(\"Positive Definite Criteria: {}\".format(algorithm))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram view of condition number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking conditioning of Matrix over episodes & comparing to true jacobian at those points\n",
    "#Focusing only on the frobenius norm\n",
    "fig, axs = plt.subplots(len(results.keys()), 1, figsize=(15, 20))\n",
    "\n",
    "for i, k_v in enumerate(results.items()):\n",
    "    algorithm, result = k_v[0], k_v[1]\n",
    "    ax = axs[i]\n",
    "    cond_hat, cond_true = check_jacobian_condition(result)\n",
    "        \n",
    "    plot_mu_sig(cond_hat, label=\"approximation\", ax=ax)\n",
    "    #plot_mu_sig(cond_true, label=\"true values\", ax=ax)\n",
    "    if i >= len(axs) - 1:\n",
    "        ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('Condition Number')\n",
    "    ax.set_title(\"Checking Condition Number: {}\".format(algorithm))\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
